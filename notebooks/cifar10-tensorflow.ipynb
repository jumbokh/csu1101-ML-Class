{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport math\nimport timeit\nimport matplotlib.pyplot as plt\nfrom six.moves import cPickle as pickle\nimport os\nimport platform\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n%matplotlib inline","metadata":{"_cell_guid":"3a672abb-a2c8-4d59-9c65-42ae2c89ac84","_uuid":"098d097dea66b981144f3aaec875f1c824f57883","execution":{"iopub.status.busy":"2021-09-08T05:00:07.238268Z","iopub.execute_input":"2021-09-08T05:00:07.238579Z","iopub.status.idle":"2021-09-08T05:00:10.620501Z","shell.execute_reply.started":"2021-09-08T05:00:07.238529Z","shell.execute_reply":"2021-09-08T05:00:10.619516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')","metadata":{"_cell_guid":"acab539e-99ab-49f8-aaa9-9c08678494ec","_uuid":"62ff7f4915898a073659a4677eb70c4a6a9e6eb4","execution":{"iopub.status.busy":"2021-09-08T05:00:10.62196Z","iopub.execute_input":"2021-09-08T05:00:10.622509Z","iopub.status.idle":"2021-09-08T05:00:10.627741Z","shell.execute_reply.started":"2021-09-08T05:00:10.622443Z","shell.execute_reply":"2021-09-08T05:00:10.627155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_pickle(f):\n    version = platform.python_version_tuple()\n    if version[0] == '2':\n        return  pickle.load(f)\n    elif version[0] == '3':\n        return  pickle.load(f, encoding='latin1')\n    raise ValueError(\"invalid python version: {}\".format(version))\n\ndef load_CIFAR_batch(filename):\n    \"\"\" load single batch of cifar \"\"\"\n    with open(filename, 'rb') as f:\n        datadict = load_pickle(f)\n        X = datadict['data']\n        Y = datadict['labels']\n        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n        Y = np.array(Y)\n        return X, Y\n\ndef load_CIFAR10(ROOT):\n    \"\"\" load all of cifar \"\"\"\n    xs = []\n    ys = []\n    for b in range(1,6):\n        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n        X, Y = load_CIFAR_batch(f)\n        xs.append(X)\n        ys.append(Y)\n    Xtr = np.concatenate(xs)\n    Ytr = np.concatenate(ys)\n    del X, Y\n    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n    return Xtr, Ytr, Xte, Yte","metadata":{"_cell_guid":"dbda6db8-29f1-4cff-9c48-24f35db15f0c","_uuid":"79098de67d87caf443e057aba591eeb3985b7a37","execution":{"iopub.status.busy":"2021-09-08T05:00:10.629125Z","iopub.execute_input":"2021-09-08T05:00:10.629742Z","iopub.status.idle":"2021-09-08T05:00:10.654937Z","shell.execute_reply.started":"2021-09-08T05:00:10.629683Z","shell.execute_reply":"2021-09-08T05:00:10.654053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n    # Load the raw CIFAR-10 data\n    cifar10_dir = '../input'\n    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n\n    # Subsample the data\n    mask = range(num_training, num_training + num_validation)\n    X_val = X_train[mask]\n    y_val = y_train[mask]\n    mask = range(num_training)\n    X_train = X_train[mask]\n    y_train = y_train[mask]\n    mask = range(num_test)\n    X_test = X_test[mask]\n    y_test = y_test[mask]\n\n    # Normalize the data: subtract the mean image\n    mean_image = np.mean(X_train, axis=0)\n    X_train -= mean_image\n    X_val -= mean_image\n    X_test -= mean_image\n\n    return X_train, y_train, X_val, y_val, X_test, y_test\n\n\n# Invoke the above function to get our data.\nX_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\nprint('Train data shape: ', X_train.shape)\nprint('Train labels shape: ', y_train.shape)\nprint('Validation data shape: ', X_val.shape)\nprint('Validation labels shape: ', y_val.shape)\nprint('Test data shape: ', X_test.shape)\nprint('Test labels shape: ', y_test.shape)","metadata":{"_cell_guid":"0c272f85-bd27-4eeb-99a7-6fd036e99344","_uuid":"53b8f7f8fb884a4c8f18350c1d4626cf198bf3fe","execution":{"iopub.status.busy":"2021-09-08T05:00:10.656079Z","iopub.execute_input":"2021-09-08T05:00:10.656506Z","iopub.status.idle":"2021-09-08T05:00:18.775762Z","shell.execute_reply.started":"2021-09-08T05:00:10.656418Z","shell.execute_reply":"2021-09-08T05:00:18.774912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define net\nclass CifarNet():\n    def __init__(self):\n        # conv layer\n        # H2 = (H1 - F + 2P)/S +1\n        # (32-5)/1 + 1 = 28\n        # 28x28x32 = 25088\n        # To ReLu (?x16x16x32) -> MaxPool (?x16x16x32) -> affine (8192)\n        self.Wconv1 = tf.get_variable(\"Wconv1\", shape=[5, 5, 3, 32])\n        self.bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n        # (32-5)/1 + 1 = 28\n        # 28x28x64 = 50176\n        self.Wconv2 = tf.get_variable(\"Wconv2\", shape=[5, 5, 32, 64])\n        self.bconv2 = tf.get_variable(\"bconv2\", shape=[64])\n        # affine layer with 1024\n        self.W1 = tf.get_variable(\"W1\", shape=[3136, 1024])\n        self.b1 = tf.get_variable(\"b1\", shape=[1024])\n        # affine layer with 10\n        self.W2 = tf.get_variable(\"W2\", shape=[1024, 10])\n        self.b2 = tf.get_variable(\"b2\", shape=[10])        \n        \n    def forward(self, X, y, is_training):\n        # conv2d\n        # ReLu\n        # conv2d\n        # ReLu\n        # maxpool\n        # Batch Norm\n        # Affine\n        # Batch Norm\n        # ReLu\n        # Affine\n        # dropout\n        # Batch Norm\n\n        # conv layer\n        # H2 = (H1 - F + 2P)/S +1\n        # (32-5)/1 + 1 = 28\n        # 28x28x32 = 25088\n        # To ReLu (?x16x16x32) -> MaxPool (?x16x16x32) -> affine (8192)\n\n        # define our graph (e.g. two_layer_convnet) with stride 1\n        conv1 = tf.nn.conv2d(X, self.Wconv1, strides=[1, 1, 1, 1], padding='SAME') + self.bconv1\n        print(conv1.shape)\n        # ReLU Activation Layer\n        relu1 = tf.nn.relu(conv1)\n        print(relu1)\n        # Conv\n        conv2 = tf.nn.conv2d(relu1, self.Wconv2, strides=[1, 2, 2, 1], padding='VALID') + self.bconv2\n        print(conv2.shape)\n        # ReLU Activation Layer\n        relu2 = tf.nn.relu(conv2)\n        print(relu2)\n        # 2x2 Max Pooling layer with a stride of 2\n        maxpool = tf.layers.max_pooling2d(relu2, pool_size=(2,2), strides=2)\n        print(maxpool.shape)\n        maxpool_flat = tf.reshape(maxpool,[-1,3136])\n        # Spatial Batch Normalization Layer (trainable parameters, with scale and centering)\n        bn1 = tf.layers.batch_normalization(inputs=maxpool_flat, center=True, scale=True, training=is_training)\n        # Affine layer with 1024 output units\n        affine1 = tf.matmul(bn1, self.W1) + self.b1\n        print(affine1.shape)\n        # vanilla batch normalization\n        affine1_flat = tf.reshape(affine1,[-1,1024])\n        bn2 = tf.layers.batch_normalization(inputs=affine1, center=True, scale=True, training=is_training)\n        print(bn2.shape)\n        # ReLU Activation Layer\n        relu2 = tf.nn.relu(bn2)\n        print(relu2.shape)\n        # dropout\n        drop1 = tf.layers.dropout(inputs=relu2, training=is_training)\n        # Affine layer from 1024 input units to 10 outputs\n        affine2 = tf.matmul(drop1, self.W2) + self.b2\n        # vanilla batch normalization\n        affine2_flat = tf.reshape(affine2,[-1,3136])\n        self.predict = tf.layers.batch_normalization(inputs=affine2, center=True, scale=True, training=is_training)\n        print(self.predict.shape)\n        return self.predict\n    \n    def run(self, session, loss_val, Xd, yd,\n                  epochs=1, batch_size=64, print_every=100,\n                  training=None, plot_losses=False, isSoftMax=False):\n        # have tensorflow compute accuracy\n        if isSoftMax:\n            correct_prediction = tf.nn.softmax(self.predict)\n        else:\n            correct_prediction = tf.equal(tf.argmax(self.predict,1), y)\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n        # shuffle indicies\n        train_indicies = np.arange(Xd.shape[0])\n        np.random.shuffle(train_indicies)\n\n        training_now = training is not None\n\n        # setting up variables we want to compute (and optimizing)\n        # if we have a training function, add that to things we compute\n        variables = [mean_loss, correct_prediction, accuracy]\n        if training_now:\n            variables[-1] = training\n\n        # counter \n        iter_cnt = 0\n        for e in range(epochs):\n            # keep track of losses and accuracy\n            correct = 0\n            losses = []\n            # make sure we iterate over the dataset once\n            for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n                # generate indicies for the batch\n                start_idx = (i*batch_size)%Xd.shape[0]\n                idx = train_indicies[start_idx:start_idx+batch_size]\n\n                # create a feed dictionary for this batch\n                feed_dict = {X: Xd[idx,:],\n                             y: yd[idx],\n                             is_training: training_now }\n                # get batch size\n                actual_batch_size = yd[idx].shape[0]\n\n                # have tensorflow compute loss and correct predictions\n                # and (if given) perform a training step\n                loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n\n                # aggregate performance stats\n                losses.append(loss*actual_batch_size)\n                correct += np.sum(corr)\n\n                # print every now and then\n                if training_now and (iter_cnt % print_every) == 0:\n                    print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n                          .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n                iter_cnt += 1\n            total_correct = correct/Xd.shape[0]\n            total_loss = np.sum(losses)/Xd.shape[0]\n            print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n                  .format(total_loss,total_correct,e+1))\n            if plot_losses:\n                plt.plot(losses)\n                plt.grid(True)\n                plt.title('Epoch {} Loss'.format(e+1))\n                plt.xlabel('minibatch number')\n                plt.ylabel('minibatch loss')\n                plt.show()\n        return total_loss, total_correct","metadata":{"_cell_guid":"0679fb2a-62be-415b-9d0c-cd901db6eaf3","_uuid":"e72e7c97da89f05d73af6932e182c82be0c9b00f","execution":{"iopub.status.busy":"2021-09-08T05:00:18.777161Z","iopub.execute_input":"2021-09-08T05:00:18.777641Z","iopub.status.idle":"2021-09-08T05:00:19.001133Z","shell.execute_reply.started":"2021-09-08T05:00:18.777588Z","shell.execute_reply":"2021-09-08T05:00:19.000516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.reset_default_graph()\nX = tf.placeholder(tf.float32, [None, 32, 32, 3])\ny = tf.placeholder(tf.int64, [None])\nis_training = tf.placeholder(tf.bool)\n\nnet = CifarNet()\nnet.forward(X,y,is_training)","metadata":{"_cell_guid":"69643b48-526b-4c32-af3e-021d44c54023","_uuid":"53a027e508704650f9350bcae18cd5a9bf78014a","execution":{"iopub.status.busy":"2021-09-08T05:00:19.002297Z","iopub.execute_input":"2021-09-08T05:00:19.002729Z","iopub.status.idle":"2021-09-08T05:00:19.310697Z","shell.execute_reply.started":"2021-09-08T05:00:19.002684Z","shell.execute_reply":"2021-09-08T05:00:19.31009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Annealing the learning rate\nglobal_step = tf.Variable(0, trainable=False)\nstarter_learning_rate = 1e-3\nend_learning_rate = 5e-3\ndecay_steps = 10000\n\nlearning_rate = tf.train.polynomial_decay(starter_learning_rate, global_step,\n                                          decay_steps, end_learning_rate,\n                                          power=0.5)\n\nexp_learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n                                               100000, 0.96, staircase=True)\n\n\n# Feel free to play with this cell\nmean_loss = None\noptimizer = None\n\n# define our loss\ncross_entr_loss = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=net.predict)\nmean_loss = tf.reduce_mean(cross_entr_loss)\n\n# define our optimizer\noptimizer = tf.train.AdamOptimizer(exp_learning_rate)\n\n\n# batch normalization in tensorflow requires this extra dependency\nextra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(extra_update_ops):\n    train_step = optimizer.minimize(mean_loss, global_step=global_step)","metadata":{"_cell_guid":"095a98f0-4822-4ed3-b44e-f290cc8b65a9","_uuid":"494f291d11d390c0901ad2c5997be2df1d575381","execution":{"iopub.status.busy":"2021-09-08T05:00:19.3116Z","iopub.execute_input":"2021-09-08T05:00:19.311914Z","iopub.status.idle":"2021-09-08T05:00:19.845917Z","shell.execute_reply.started":"2021-09-08T05:00:19.311879Z","shell.execute_reply":"2021-09-08T05:00:19.845373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train with 10 epochs\nsess = tf.Session()\n\ntry:\n    with tf.device(\"/cpu:0\") as dev:\n        sess.run(tf.global_variables_initializer())\n        print('Training')\n        net.run(sess, mean_loss, X_train, y_train, 10, 64, 200, train_step, True)\n        print('Validation')\n        net.run(sess, mean_loss, X_val, y_val, 1, 64)\nexcept tf.errors.InvalidArgumentError:\n    print(\"no gpu found, please use Google Cloud if you want GPU acceleration\") ","metadata":{"_cell_guid":"d2d71560-1af3-4eed-9ab7-f2e992fdd0f3","_uuid":"d0bc3aca85023f3dfb4883d3205a2fdc78dbff22","execution":{"iopub.status.busy":"2021-09-08T05:00:19.847093Z","iopub.execute_input":"2021-09-08T05:00:19.847586Z","iopub.status.idle":"2021-09-08T05:37:46.166748Z","shell.execute_reply.started":"2021-09-08T05:00:19.84753Z","shell.execute_reply":"2021-09-08T05:37:46.16572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view net model result on train  and validation set\nprint('Training')\nnet.run(sess, mean_loss, X_train, y_train, 1, 64)\nprint('Validation')\nnet.run(sess, mean_loss, X_val, y_val, 1, 64)","metadata":{"_cell_guid":"cc907e4d-0ed7-4fdc-a726-cec27800948d","_uuid":"fffadbfd91162500d980cd79d636e06ccc0cd3a6","execution":{"iopub.status.busy":"2021-09-08T05:37:46.168776Z","iopub.execute_input":"2021-09-08T05:37:46.16903Z","iopub.status.idle":"2021-09-08T05:39:09.440296Z","shell.execute_reply.started":"2021-09-08T05:37:46.168995Z","shell.execute_reply":"2021-09-08T05:39:09.439481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check result on test\nprint('Test')\nnet.run(sess, mean_loss, X_test, y_test, 1, 64)","metadata":{"_cell_guid":"49fe1733-ec19-4919-a76f-b4260a114cd0","_uuid":"685c202406f21105eb173eb92f4cc22dbe7c476f","execution":{"iopub.status.busy":"2021-09-08T05:39:09.441494Z","iopub.execute_input":"2021-09-08T05:39:09.441796Z","iopub.status.idle":"2021-09-08T05:39:26.720124Z","shell.execute_reply.started":"2021-09-08T05:39:09.441745Z","shell.execute_reply":"2021-09-08T05:39:26.719342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a feed dictionary for this batch\nfeed_dict = {X: X_test,\n             y: y_test,\n             is_training: False}\n\n# predict\npredict = sess.run(tf.nn.softmax(net.predict), feed_dict=feed_dict)\npredict_df = pd.DataFrame(predict, columns=classes)","metadata":{"_cell_guid":"3bfa901e-67d1-4ca9-aa90-bc4ff6fdc0ff","_uuid":"e242183c55e18297d3ba816bad22a2cc09190b62","execution":{"iopub.status.busy":"2021-09-08T05:39:26.72137Z","iopub.execute_input":"2021-09-08T05:39:26.721679Z","iopub.status.idle":"2021-09-08T05:39:45.405088Z","shell.execute_reply.started":"2021-09-08T05:39:26.721626Z","shell.execute_reply":"2021-09-08T05:39:45.404492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_df.head()","metadata":{"_cell_guid":"f121fa99-bbc8-4e7d-aa06-41e712d781cb","_uuid":"f28553b32154ae1186e7e050dbf308da4630e2af","execution":{"iopub.status.busy":"2021-09-08T05:39:45.405966Z","iopub.execute_input":"2021-09-08T05:39:45.406301Z","iopub.status.idle":"2021-09-08T05:39:45.436341Z","shell.execute_reply.started":"2021-09-08T05:39:45.406253Z","shell.execute_reply":"2021-09-08T05:39:45.435749Z"},"trusted":true},"execution_count":null,"outputs":[]}]}